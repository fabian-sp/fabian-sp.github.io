---
permalink: /
title: "Home"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
-------------
I am a researcher working on Optimization for Machine Learning. Since September 2024, I am a postdoc at Inria Paris, advised by Francis Bach, Umut Simsekli, and Adrien Taylor. I obtained my PhD at TUM (Technical University of Munich), supervised by Professor Michael Ulbrich. 

You can contact me at `firsstname.lastname@tum.de`.

My research interests are mainly stochastic optimization algorithms for machine learning, and related topics in computation and statistics. One common theme of my past work is trying to understand and simplify training recipes for ML models. Research overview:

* Understanding the behaviour of learning-rate schedules for LLM training ([link to paper](https://arxiv.org/pdf/2501.18965))
*  Practical adaptive learning-rate methods based on the Polyak step size ([**MoMo**](https://arxiv.org/abs/2305.07583), [**ProxSPS**](https://openreview.net/forum?id=jWr41htaB3)). This was one of the main topics of my PhD.
* Most recently: scaling laws for LLM training from an optimization theory perspective

News
----------

* June/July 2025: I will present at EUROPT 2025 in Southhampton and at ICML in Vancouver.
* December 2024: Attended NeurIPS@Paris at Sorbonne.
* July 2024: I successfully defended my PhD thesis!
* August - October 2023: Visiting researcher at CCM, Flatiron Institute, New York City.
* June 2023: Participated in the [ProbAI summer schol](https://probabilistic.ai/) in Trondheim, Norway.