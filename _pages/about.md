---
permalink: /
title: "Home"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
-------------
I am a PhD student in mathematical optimization at TUM (Technical University of Munich), supervised by Professor Michael Ulbrich. My interests surround optimization in general, and more specifically stochastic optimization algorithms for machine learning and related topics in computation and statistics. My email is firsstname.lastname@tum.de.

My recent work focuses on adaptive learning rates for algorithms such as SGD or Adam, and how they can reduce learning rate tuning. In this line of work, we developped (i) a way to include regularization functions in stochastic Polyak stepsizes (**ProxSPS**, [link to paper](https://openreview.net/forum?id=jWr41htaB3)), and (ii) how to combine adaptive learning rates with any preconditioned momentum method such as SGD-M an Adam (**MoMo**, [link to paper](https://arxiv.org/abs/2305.07583)). 

News
----------

* August - October 2023: Visiting researcher at CCM, Flatiron Institute, New York City.
* June 2023: Participated in the [ProbAI summer schol](https://probabilistic.ai/) in Trondheim, Norway.