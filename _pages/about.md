---
permalink: /
title: "Home"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

About me
-------------
I am a researcher working on Optimization for Machine Learning. Currently, I am a postdoc at Inria Paris, advised by Francis Bach, Umut Simsekli, and Adrien Taylor. I obtained my PhD at TUM (Technical University of Munich), supervised by Professor Michael Ulbrich. You can contact me at `firsstname.lastname@tum.de`.

My research interests are mainly stochastic optimization algorithms for machine learning, and related topics in computation and statistics. One common theme of my research is trying to understand how to simplify training recipes for ML models, in particular how to reduce the tuning effort for algorithms like SGD or Adam. 

Most recently, we studied the behaviour of learning-rate schedules for LLM training ([link to paper](https://arxiv.org/pdf/2501.18965)). During my PhD I mostly worked on *practical* adaptive learning-rate methods based on the Polyak step size ([**MoMo**](https://arxiv.org/abs/2305.07583), [**ProxSPS**](https://openreview.net/forum?id=jWr41htaB3)).

News
----------

* December 2024: I will be at NeurIPS@Paris at Sorbonne.
* July 2024: I successfully defended my PhD thesis!
* August - October 2023: Visiting researcher at CCM, Flatiron Institute, New York City.
* June 2023: Participated in the [ProbAI summer schol](https://probabilistic.ai/) in Trondheim, Norway.