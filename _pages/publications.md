---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

* The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training, Schaipp F, Hägele A, Taylor A, Simsekli U, Bach F. **ICML 2025**.
[[abs]](https://arxiv.org/abs/2501.18965) [[pdf]](https://arxiv.org/pdf/2501.18965)

* MoMo: Momentum Models for Adaptive Learning Rates, Schaipp F, Ohana R, Eickenberg M, Defazio A, Gower R. **ICML 2024**. [[abs]](https://arxiv.org/abs/2305.07583) [[pdf]](https://arxiv.org/pdf/2305.07583) [[poster]](/files/poster_momo.pdf)

* A Semismooth Newton Stochastic Proximal Point Algorithm with Variance Reduction, Milzarek A, Schaipp F, Ulbrich M. **SIOPT 2024**. [[abs]](https://epubs.siam.org/doi/abs/10.1137/22M1488181)  [[pdf]](https://arxiv.org/pdf/2204.00406)

* SGD with Clipping is Secretly Estimating the Median Gradient, Schaipp F, Garrigos G, Simsekli U, Gower R. Arxiv preprint 2024. [[abs]](https://arxiv.org/abs/2402.12828) [[pdf]](https://arxiv.org/pdf/2402.12828)

* Robust gradient estimation in the presence of heavy-tailed noise , Schaipp F, Simsekli U, Gower R. **NeurIPS Workshop Heavy Tails in Machine Learning 2023**.[[abs]](https://openreview.net/forum?id=C6PiH9Fkjd) [[pdf]](https://openreview.net/pdf?id=C6PiH9Fkjd)

* A Stochastic Proximal Polyak Step Size, Schaipp F, Gower R, Ulbrich M. **TMLR 2023**. [[abs]](https://openreview.net/forum?id=jWr41htaB3) [[pdf]](https://openreview.net/pdf?id=jWr41htaB3)

* GGLasso - a Python package for General Graphical Lasso computation, Schaipp F, Vlasovets O, Müller C. **JOSS 2021**. [[abs]](https://joss.theoj.org/papers/10.21105/joss.03865) [[pdf]](https://www.theoj.org/joss-papers/joss.03865/10.21105.joss.03865.pdf)
